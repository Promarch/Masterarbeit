gpt:
  block_size: 16
  vocab_size: 2048
  num_layers: 12
  num_heads: 12
  num_embeds: 768
  dropout_rate: 0.1
  use_bias: True
  dtype: float32

enc:
  mu: 255

epochs: 5
batch_size: 256
lr: 1.0e-4
train_test_split: 0.9
data_path: ./res/transitions_dataset.pkl
api_path: ./wandbkey.txt
project_name: gptlearner
mode: params
checkpoint_path: ./res/model.pkl
use_wandb: False
train: true